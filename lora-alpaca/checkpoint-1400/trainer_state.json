{
  "best_metric": 0.9379881620407104,
  "best_model_checkpoint": "./lora-alpaca/checkpoint-200",
  "epoch": 3.5837133029357653,
  "eval_steps": 200,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.025597952163826893,
      "grad_norm": 1.0350425243377686,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 2.3021,
      "step": 10
    },
    {
      "epoch": 0.051195904327653786,
      "grad_norm": 1.1547452211380005,
      "learning_rate": 5.1e-05,
      "loss": 2.2697,
      "step": 20
    },
    {
      "epoch": 0.07679385649148068,
      "grad_norm": 0.9265931248664856,
      "learning_rate": 8.1e-05,
      "loss": 2.0858,
      "step": 30
    },
    {
      "epoch": 0.10239180865530757,
      "grad_norm": 2.9043872356414795,
      "learning_rate": 0.00011099999999999999,
      "loss": 1.6353,
      "step": 40
    },
    {
      "epoch": 0.12798976081913446,
      "grad_norm": 0.5574853420257568,
      "learning_rate": 0.00014099999999999998,
      "loss": 1.2467,
      "step": 50
    },
    {
      "epoch": 0.15358771298296137,
      "grad_norm": 0.2837185263633728,
      "learning_rate": 0.00017099999999999998,
      "loss": 1.1427,
      "step": 60
    },
    {
      "epoch": 0.17918566514678827,
      "grad_norm": 0.20404893159866333,
      "learning_rate": 0.000201,
      "loss": 1.0772,
      "step": 70
    },
    {
      "epoch": 0.20478361731061515,
      "grad_norm": 0.15627753734588623,
      "learning_rate": 0.00022799999999999999,
      "loss": 1.0463,
      "step": 80
    },
    {
      "epoch": 0.23038156947444205,
      "grad_norm": 0.1323925107717514,
      "learning_rate": 0.000258,
      "loss": 1.0344,
      "step": 90
    },
    {
      "epoch": 0.2559795216382689,
      "grad_norm": 0.18269822001457214,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.0123,
      "step": 100
    },
    {
      "epoch": 0.28157747380209586,
      "grad_norm": 0.2543672025203705,
      "learning_rate": 0.000298972602739726,
      "loss": 1.0217,
      "step": 110
    },
    {
      "epoch": 0.30717542596592273,
      "grad_norm": 0.1811225265264511,
      "learning_rate": 0.00029712328767123286,
      "loss": 1.0025,
      "step": 120
    },
    {
      "epoch": 0.3327733781297496,
      "grad_norm": 40.16688537597656,
      "learning_rate": 0.0002950684931506849,
      "loss": 0.9993,
      "step": 130
    },
    {
      "epoch": 0.35837133029357654,
      "grad_norm": 0.546116292476654,
      "learning_rate": 0.000293013698630137,
      "loss": 0.9957,
      "step": 140
    },
    {
      "epoch": 0.3839692824574034,
      "grad_norm": 0.2365034818649292,
      "learning_rate": 0.000290958904109589,
      "loss": 0.9677,
      "step": 150
    },
    {
      "epoch": 0.4095672346212303,
      "grad_norm": 0.14739061892032623,
      "learning_rate": 0.00028890410958904106,
      "loss": 0.9567,
      "step": 160
    },
    {
      "epoch": 0.4351651867850572,
      "grad_norm": 0.1498708426952362,
      "learning_rate": 0.0002868493150684931,
      "loss": 0.9598,
      "step": 170
    },
    {
      "epoch": 0.4607631389488841,
      "grad_norm": 0.1390378773212433,
      "learning_rate": 0.0002847945205479452,
      "loss": 0.9374,
      "step": 180
    },
    {
      "epoch": 0.486361091112711,
      "grad_norm": 0.14319175481796265,
      "learning_rate": 0.0002827397260273972,
      "loss": 0.9513,
      "step": 190
    },
    {
      "epoch": 0.5119590432765379,
      "grad_norm": 0.1500396430492401,
      "learning_rate": 0.00028068493150684926,
      "loss": 0.9095,
      "step": 200
    },
    {
      "epoch": 0.5119590432765379,
      "eval_loss": 0.9379881620407104,
      "eval_runtime": 72.7675,
      "eval_samples_per_second": 27.485,
      "eval_steps_per_second": 3.436,
      "step": 200
    },
    {
      "epoch": 0.5375569954403647,
      "grad_norm": 0.1992788016796112,
      "learning_rate": 0.00027863013698630135,
      "loss": 0.9251,
      "step": 210
    },
    {
      "epoch": 0.5631549476041917,
      "grad_norm": 0.1755039095878601,
      "learning_rate": 0.0002765753424657534,
      "loss": 0.9455,
      "step": 220
    },
    {
      "epoch": 0.5887528997680186,
      "grad_norm": 0.15560391545295715,
      "learning_rate": 0.0002745205479452055,
      "loss": 0.9138,
      "step": 230
    },
    {
      "epoch": 0.6143508519318455,
      "grad_norm": 0.16925324499607086,
      "learning_rate": 0.0002724657534246575,
      "loss": 0.9208,
      "step": 240
    },
    {
      "epoch": 0.6399488040956723,
      "grad_norm": 0.13569580018520355,
      "learning_rate": 0.00027041095890410955,
      "loss": 0.929,
      "step": 250
    },
    {
      "epoch": 0.6655467562594992,
      "grad_norm": 0.14024057984352112,
      "learning_rate": 0.00026835616438356164,
      "loss": 0.924,
      "step": 260
    },
    {
      "epoch": 0.6911447084233261,
      "grad_norm": 0.17237554490566254,
      "learning_rate": 0.0002663013698630137,
      "loss": 0.9209,
      "step": 270
    },
    {
      "epoch": 0.7167426605871531,
      "grad_norm": 0.15476205945014954,
      "learning_rate": 0.0002642465753424657,
      "loss": 0.9201,
      "step": 280
    },
    {
      "epoch": 0.74234061275098,
      "grad_norm": 0.16795168817043304,
      "learning_rate": 0.00026219178082191775,
      "loss": 0.9062,
      "step": 290
    },
    {
      "epoch": 0.7679385649148068,
      "grad_norm": 0.18415077030658722,
      "learning_rate": 0.00026013698630136985,
      "loss": 0.8981,
      "step": 300
    },
    {
      "epoch": 0.7935365170786337,
      "grad_norm": 0.16133534908294678,
      "learning_rate": 0.0002580821917808219,
      "loss": 0.9097,
      "step": 310
    },
    {
      "epoch": 0.8191344692424606,
      "grad_norm": 0.15385326743125916,
      "learning_rate": 0.00025602739726027397,
      "loss": 0.9228,
      "step": 320
    },
    {
      "epoch": 0.8447324214062875,
      "grad_norm": 0.26558127999305725,
      "learning_rate": 0.000253972602739726,
      "loss": 0.9219,
      "step": 330
    },
    {
      "epoch": 0.8703303735701144,
      "grad_norm": 0.5433973073959351,
      "learning_rate": 0.0002519178082191781,
      "loss": 0.9238,
      "step": 340
    },
    {
      "epoch": 0.8959283257339413,
      "grad_norm": 0.3430865705013275,
      "learning_rate": 0.00024986301369863014,
      "loss": 0.9354,
      "step": 350
    },
    {
      "epoch": 0.9215262778977682,
      "grad_norm": 4.375153064727783,
      "learning_rate": 0.0002478082191780822,
      "loss": 0.9433,
      "step": 360
    },
    {
      "epoch": 0.9471242300615951,
      "grad_norm": 46.579586029052734,
      "learning_rate": 0.0002457534246575342,
      "loss": 1.0513,
      "step": 370
    },
    {
      "epoch": 0.972722182225422,
      "grad_norm": 113.54442596435547,
      "learning_rate": 0.00024410958904109588,
      "loss": 2.8203,
      "step": 380
    },
    {
      "epoch": 0.9983201343892488,
      "grad_norm": 15.886500358581543,
      "learning_rate": 0.00024205479452054792,
      "loss": 2.8564,
      "step": 390
    },
    {
      "epoch": 1.0239180865530757,
      "grad_norm": 0.35477301478385925,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.3367,
      "step": 400
    },
    {
      "epoch": 1.0239180865530757,
      "eval_loss": 0.9544815421104431,
      "eval_runtime": 73.1254,
      "eval_samples_per_second": 27.35,
      "eval_steps_per_second": 3.419,
      "step": 400
    },
    {
      "epoch": 1.0495160387169027,
      "grad_norm": 0.5288390517234802,
      "learning_rate": 0.00023794520547945204,
      "loss": 0.9375,
      "step": 410
    },
    {
      "epoch": 1.0751139908807295,
      "grad_norm": 4.26974630355835,
      "learning_rate": 0.00023589041095890408,
      "loss": 0.9486,
      "step": 420
    },
    {
      "epoch": 1.1007119430445564,
      "grad_norm": 3378.35302734375,
      "learning_rate": 0.00023424657534246575,
      "loss": 1.9728,
      "step": 430
    },
    {
      "epoch": 1.1263098952083834,
      "grad_norm": 309.332763671875,
      "learning_rate": 0.00023219178082191778,
      "loss": 4.0086,
      "step": 440
    },
    {
      "epoch": 1.1519078473722102,
      "grad_norm": 2427.65966796875,
      "learning_rate": 0.00023054794520547942,
      "loss": 5.1661,
      "step": 450
    },
    {
      "epoch": 1.1775057995360372,
      "grad_norm": 49424.25,
      "learning_rate": 0.0002284931506849315,
      "loss": 6.8498,
      "step": 460
    },
    {
      "epoch": 1.203103751699864,
      "grad_norm": NaN,
      "learning_rate": 0.00022726027397260273,
      "loss": 7.637,
      "step": 470
    },
    {
      "epoch": 1.228701703863691,
      "grad_norm": 11.6922607421875,
      "learning_rate": 0.0002254109589041096,
      "loss": 7.6009,
      "step": 480
    },
    {
      "epoch": 1.2542996560275177,
      "grad_norm": 3.293531894683838,
      "learning_rate": 0.00022335616438356163,
      "loss": 4.6914,
      "step": 490
    },
    {
      "epoch": 1.2798976081913447,
      "grad_norm": 1.2409127950668335,
      "learning_rate": 0.00022130136986301367,
      "loss": 2.4188,
      "step": 500
    },
    {
      "epoch": 1.3054955603551717,
      "grad_norm": 1.5062859058380127,
      "learning_rate": 0.00021924657534246573,
      "loss": 1.5861,
      "step": 510
    },
    {
      "epoch": 1.3310935125189984,
      "grad_norm": 0.32351967692375183,
      "learning_rate": 0.0002171917808219178,
      "loss": 1.1701,
      "step": 520
    },
    {
      "epoch": 1.3566914646828254,
      "grad_norm": 0.23804771900177002,
      "learning_rate": 0.00021513698630136986,
      "loss": 1.0388,
      "step": 530
    },
    {
      "epoch": 1.3822894168466522,
      "grad_norm": 0.2078215628862381,
      "learning_rate": 0.0002130821917808219,
      "loss": 0.9878,
      "step": 540
    },
    {
      "epoch": 1.4078873690104792,
      "grad_norm": 0.1846720278263092,
      "learning_rate": 0.00021102739726027394,
      "loss": 0.9917,
      "step": 550
    },
    {
      "epoch": 1.433485321174306,
      "grad_norm": 0.17549213767051697,
      "learning_rate": 0.00020897260273972603,
      "loss": 0.9737,
      "step": 560
    },
    {
      "epoch": 1.459083273338133,
      "grad_norm": 0.1749488115310669,
      "learning_rate": 0.00020691780821917806,
      "loss": 0.9699,
      "step": 570
    },
    {
      "epoch": 1.48468122550196,
      "grad_norm": 0.16458947956562042,
      "learning_rate": 0.00020486301369863013,
      "loss": 0.9549,
      "step": 580
    },
    {
      "epoch": 1.5102791776657867,
      "grad_norm": 0.16102257370948792,
      "learning_rate": 0.00020280821917808216,
      "loss": 0.9621,
      "step": 590
    },
    {
      "epoch": 1.5358771298296137,
      "grad_norm": 0.21104247868061066,
      "learning_rate": 0.00020075342465753426,
      "loss": 0.9472,
      "step": 600
    },
    {
      "epoch": 1.5358771298296137,
      "eval_loss": 0.9675114750862122,
      "eval_runtime": 73.4721,
      "eval_samples_per_second": 27.221,
      "eval_steps_per_second": 3.403,
      "step": 600
    },
    {
      "epoch": 1.5614750819934407,
      "grad_norm": 0.1480187624692917,
      "learning_rate": 0.0001986986301369863,
      "loss": 0.9579,
      "step": 610
    },
    {
      "epoch": 1.5870730341572674,
      "grad_norm": 0.18624155223369598,
      "learning_rate": 0.00019664383561643833,
      "loss": 0.9526,
      "step": 620
    },
    {
      "epoch": 1.6126709863210942,
      "grad_norm": 0.2067164182662964,
      "learning_rate": 0.0001945890410958904,
      "loss": 0.9633,
      "step": 630
    },
    {
      "epoch": 1.6382689384849212,
      "grad_norm": 0.35993492603302,
      "learning_rate": 0.00019253424657534243,
      "loss": 0.9478,
      "step": 640
    },
    {
      "epoch": 1.6638668906487482,
      "grad_norm": 3.8872153759002686,
      "learning_rate": 0.00019047945205479452,
      "loss": 0.9625,
      "step": 650
    },
    {
      "epoch": 1.689464842812575,
      "grad_norm": 2.373220682144165,
      "learning_rate": 0.00018842465753424656,
      "loss": 0.9732,
      "step": 660
    },
    {
      "epoch": 1.715062794976402,
      "grad_norm": 6.718731880187988,
      "learning_rate": 0.0001863698630136986,
      "loss": 0.9753,
      "step": 670
    },
    {
      "epoch": 1.740660747140229,
      "grad_norm": 8.676791191101074,
      "learning_rate": 0.00018431506849315066,
      "loss": 0.9857,
      "step": 680
    },
    {
      "epoch": 1.7662586993040557,
      "grad_norm": 46.385894775390625,
      "learning_rate": 0.00018226027397260272,
      "loss": 1.0038,
      "step": 690
    },
    {
      "epoch": 1.7918566514678824,
      "grad_norm": 54.532840728759766,
      "learning_rate": 0.00018020547945205479,
      "loss": 1.0433,
      "step": 700
    },
    {
      "epoch": 1.8174546036317094,
      "grad_norm": 142.9502410888672,
      "learning_rate": 0.00017815068493150682,
      "loss": 1.0374,
      "step": 710
    },
    {
      "epoch": 1.8430525557955364,
      "grad_norm": 554.5499877929688,
      "learning_rate": 0.00017609589041095886,
      "loss": 0.9892,
      "step": 720
    },
    {
      "epoch": 1.8686505079593632,
      "grad_norm": 130.4555206298828,
      "learning_rate": 0.00017404109589041095,
      "loss": 0.987,
      "step": 730
    },
    {
      "epoch": 1.8942484601231901,
      "grad_norm": 112.53375244140625,
      "learning_rate": 0.000171986301369863,
      "loss": 0.9962,
      "step": 740
    },
    {
      "epoch": 1.9198464122870171,
      "grad_norm": 205.8436279296875,
      "learning_rate": 0.00016993150684931505,
      "loss": 0.9963,
      "step": 750
    },
    {
      "epoch": 1.945444364450844,
      "grad_norm": 336.00286865234375,
      "learning_rate": 0.0001678767123287671,
      "loss": 0.9983,
      "step": 760
    },
    {
      "epoch": 1.9710423166146707,
      "grad_norm": 178.09921264648438,
      "learning_rate": 0.00016582191780821918,
      "loss": 1.0036,
      "step": 770
    },
    {
      "epoch": 1.9966402687784979,
      "grad_norm": 25.808853149414062,
      "learning_rate": 0.00016376712328767122,
      "loss": 1.0193,
      "step": 780
    },
    {
      "epoch": 2.0222382209423246,
      "grad_norm": 559.9446411132812,
      "learning_rate": 0.00016171232876712328,
      "loss": 1.015,
      "step": 790
    },
    {
      "epoch": 2.0478361731061514,
      "grad_norm": 996.4069213867188,
      "learning_rate": 0.00015965753424657532,
      "loss": 1.0132,
      "step": 800
    },
    {
      "epoch": 2.0478361731061514,
      "eval_loss": 1.0143245458602905,
      "eval_runtime": 73.0919,
      "eval_samples_per_second": 27.363,
      "eval_steps_per_second": 3.42,
      "step": 800
    },
    {
      "epoch": 2.0734341252699786,
      "grad_norm": 49.32901382446289,
      "learning_rate": 0.0001576027397260274,
      "loss": 1.0452,
      "step": 810
    },
    {
      "epoch": 2.0990320774338054,
      "grad_norm": 45.49182891845703,
      "learning_rate": 0.00015554794520547944,
      "loss": 1.0568,
      "step": 820
    },
    {
      "epoch": 2.124630029597632,
      "grad_norm": 15397.3095703125,
      "learning_rate": 0.00015349315068493148,
      "loss": 1.0328,
      "step": 830
    },
    {
      "epoch": 2.150227981761459,
      "grad_norm": 6305.3759765625,
      "learning_rate": 0.00015143835616438354,
      "loss": 1.1289,
      "step": 840
    },
    {
      "epoch": 2.175825933925286,
      "grad_norm": 17329.751953125,
      "learning_rate": 0.0001493835616438356,
      "loss": 1.0799,
      "step": 850
    },
    {
      "epoch": 2.201423886089113,
      "grad_norm": 13738.1953125,
      "learning_rate": 0.00014732876712328767,
      "loss": 1.0448,
      "step": 860
    },
    {
      "epoch": 2.2270218382529396,
      "grad_norm": 69.48270416259766,
      "learning_rate": 0.0001452739726027397,
      "loss": 1.0685,
      "step": 870
    },
    {
      "epoch": 2.252619790416767,
      "grad_norm": 1430.7392578125,
      "learning_rate": 0.00014321917808219177,
      "loss": 1.091,
      "step": 880
    },
    {
      "epoch": 2.2782177425805936,
      "grad_norm": 33416.1796875,
      "learning_rate": 0.0001411643835616438,
      "loss": 1.1465,
      "step": 890
    },
    {
      "epoch": 2.3038156947444204,
      "grad_norm": 1711.9287109375,
      "learning_rate": 0.00013910958904109587,
      "loss": 1.4372,
      "step": 900
    },
    {
      "epoch": 2.329413646908247,
      "grad_norm": 15085.3056640625,
      "learning_rate": 0.00013705479452054794,
      "loss": 2.3096,
      "step": 910
    },
    {
      "epoch": 2.3550115990720744,
      "grad_norm": 2869.6064453125,
      "learning_rate": 0.000135,
      "loss": 1.8908,
      "step": 920
    },
    {
      "epoch": 2.380609551235901,
      "grad_norm": 5185.0634765625,
      "learning_rate": 0.00013294520547945204,
      "loss": 3.0728,
      "step": 930
    },
    {
      "epoch": 2.406207503399728,
      "grad_norm": 1206.394775390625,
      "learning_rate": 0.0001308904109589041,
      "loss": 5.9332,
      "step": 940
    },
    {
      "epoch": 2.431805455563555,
      "grad_norm": 3478.9208984375,
      "learning_rate": 0.00012883561643835614,
      "loss": 4.6545,
      "step": 950
    },
    {
      "epoch": 2.457403407727382,
      "grad_norm": 4632.33349609375,
      "learning_rate": 0.0001267808219178082,
      "loss": 5.305,
      "step": 960
    },
    {
      "epoch": 2.4830013598912086,
      "grad_norm": 1297.8970947265625,
      "learning_rate": 0.00012472602739726027,
      "loss": 5.4297,
      "step": 970
    },
    {
      "epoch": 2.5085993120550354,
      "grad_norm": 541.0225830078125,
      "learning_rate": 0.00012267123287671233,
      "loss": 4.887,
      "step": 980
    },
    {
      "epoch": 2.5341972642188626,
      "grad_norm": 4708.7041015625,
      "learning_rate": 0.00012061643835616437,
      "loss": 4.1006,
      "step": 990
    },
    {
      "epoch": 2.5597952163826894,
      "grad_norm": 6629.2939453125,
      "learning_rate": 0.00011856164383561643,
      "loss": 3.8954,
      "step": 1000
    },
    {
      "epoch": 2.5597952163826894,
      "eval_loss": 3.885894536972046,
      "eval_runtime": 71.6525,
      "eval_samples_per_second": 27.913,
      "eval_steps_per_second": 3.489,
      "step": 1000
    },
    {
      "epoch": 2.585393168546516,
      "grad_norm": 5595.00927734375,
      "learning_rate": 0.00011650684931506848,
      "loss": 4.0049,
      "step": 1010
    },
    {
      "epoch": 2.6109911207103433,
      "grad_norm": 1400.93212890625,
      "learning_rate": 0.00011445205479452055,
      "loss": 4.4297,
      "step": 1020
    },
    {
      "epoch": 2.63658907287417,
      "grad_norm": 4376.71630859375,
      "learning_rate": 0.00011239726027397258,
      "loss": 4.7484,
      "step": 1030
    },
    {
      "epoch": 2.662187025037997,
      "grad_norm": 117.99278259277344,
      "learning_rate": 0.00011034246575342465,
      "loss": 6.236,
      "step": 1040
    },
    {
      "epoch": 2.687784977201824,
      "grad_norm": 44.125980377197266,
      "learning_rate": 0.0001082876712328767,
      "loss": 3.614,
      "step": 1050
    },
    {
      "epoch": 2.713382929365651,
      "grad_norm": 399.03924560546875,
      "learning_rate": 0.00010623287671232876,
      "loss": 2.3884,
      "step": 1060
    },
    {
      "epoch": 2.7389808815294776,
      "grad_norm": 39319.23828125,
      "learning_rate": 0.00010417808219178081,
      "loss": 2.0647,
      "step": 1070
    },
    {
      "epoch": 2.7645788336933044,
      "grad_norm": 58504.73046875,
      "learning_rate": 0.00010212328767123287,
      "loss": 6.1702,
      "step": 1080
    },
    {
      "epoch": 2.7901767858571316,
      "grad_norm": 18358.482421875,
      "learning_rate": 0.00010006849315068493,
      "loss": 8.97,
      "step": 1090
    },
    {
      "epoch": 2.8157747380209583,
      "grad_norm": 3689.159912109375,
      "learning_rate": 9.801369863013699e-05,
      "loss": 6.129,
      "step": 1100
    },
    {
      "epoch": 2.841372690184785,
      "grad_norm": 11305.923828125,
      "learning_rate": 9.595890410958903e-05,
      "loss": 4.7795,
      "step": 1110
    },
    {
      "epoch": 2.866970642348612,
      "grad_norm": 27674.91796875,
      "learning_rate": 9.390410958904109e-05,
      "loss": 4.7462,
      "step": 1120
    },
    {
      "epoch": 2.892568594512439,
      "grad_norm": 48518.6171875,
      "learning_rate": 9.184931506849314e-05,
      "loss": 4.5586,
      "step": 1130
    },
    {
      "epoch": 2.918166546676266,
      "grad_norm": 5084.154296875,
      "learning_rate": 8.97945205479452e-05,
      "loss": 5.2704,
      "step": 1140
    },
    {
      "epoch": 2.9437644988400926,
      "grad_norm": 2771.6396484375,
      "learning_rate": 8.773972602739725e-05,
      "loss": 6.5621,
      "step": 1150
    },
    {
      "epoch": 2.96936245100392,
      "grad_norm": 3161.36669921875,
      "learning_rate": 8.568493150684932e-05,
      "loss": 6.7215,
      "step": 1160
    },
    {
      "epoch": 2.9949604031677466,
      "grad_norm": 9098.5791015625,
      "learning_rate": 8.363013698630135e-05,
      "loss": 5.6389,
      "step": 1170
    },
    {
      "epoch": 3.0205583553315734,
      "grad_norm": 12978.6611328125,
      "learning_rate": 8.157534246575342e-05,
      "loss": 6.0308,
      "step": 1180
    },
    {
      "epoch": 3.0461563074954006,
      "grad_norm": 1936.46923828125,
      "learning_rate": 7.952054794520547e-05,
      "loss": 5.6479,
      "step": 1190
    },
    {
      "epoch": 3.0717542596592273,
      "grad_norm": 10069.2978515625,
      "learning_rate": 7.746575342465753e-05,
      "loss": 5.2266,
      "step": 1200
    },
    {
      "epoch": 3.0717542596592273,
      "eval_loss": 4.8148722648620605,
      "eval_runtime": 71.357,
      "eval_samples_per_second": 28.028,
      "eval_steps_per_second": 3.504,
      "step": 1200
    },
    {
      "epoch": 3.097352211823054,
      "grad_norm": 3516.0693359375,
      "learning_rate": 7.541095890410958e-05,
      "loss": 5.4023,
      "step": 1210
    },
    {
      "epoch": 3.122950163986881,
      "grad_norm": 20353.53125,
      "learning_rate": 7.335616438356163e-05,
      "loss": 4.6784,
      "step": 1220
    },
    {
      "epoch": 3.148548116150708,
      "grad_norm": 7409.17626953125,
      "learning_rate": 7.13013698630137e-05,
      "loss": 4.3715,
      "step": 1230
    },
    {
      "epoch": 3.174146068314535,
      "grad_norm": 1674.8255615234375,
      "learning_rate": 6.924657534246575e-05,
      "loss": 4.1356,
      "step": 1240
    },
    {
      "epoch": 3.1997440204783616,
      "grad_norm": 210.80581665039062,
      "learning_rate": 6.71917808219178e-05,
      "loss": 3.7322,
      "step": 1250
    },
    {
      "epoch": 3.225341972642189,
      "grad_norm": 73.9555435180664,
      "learning_rate": 6.513698630136986e-05,
      "loss": 2.8929,
      "step": 1260
    },
    {
      "epoch": 3.2509399248060156,
      "grad_norm": 684.0548095703125,
      "learning_rate": 6.308219178082191e-05,
      "loss": 2.2357,
      "step": 1270
    },
    {
      "epoch": 3.2765378769698423,
      "grad_norm": 1194.8681640625,
      "learning_rate": 6.102739726027397e-05,
      "loss": 1.786,
      "step": 1280
    },
    {
      "epoch": 3.302135829133669,
      "grad_norm": 32705.255859375,
      "learning_rate": 5.897260273972602e-05,
      "loss": 1.7163,
      "step": 1290
    },
    {
      "epoch": 3.3277337812974963,
      "grad_norm": 517564.625,
      "learning_rate": 5.691780821917808e-05,
      "loss": 1.8017,
      "step": 1300
    },
    {
      "epoch": 3.353331733461323,
      "grad_norm": 65382.05078125,
      "learning_rate": 5.506849315068492e-05,
      "loss": 1.877,
      "step": 1310
    },
    {
      "epoch": 3.37892968562515,
      "grad_norm": 43663.67578125,
      "learning_rate": 5.301369863013698e-05,
      "loss": 1.9293,
      "step": 1320
    },
    {
      "epoch": 3.404527637788977,
      "grad_norm": 13033.13671875,
      "learning_rate": 5.0958904109589037e-05,
      "loss": 1.987,
      "step": 1330
    },
    {
      "epoch": 3.430125589952804,
      "grad_norm": 57668.65625,
      "learning_rate": 4.8904109589041094e-05,
      "loss": 2.0774,
      "step": 1340
    },
    {
      "epoch": 3.4557235421166306,
      "grad_norm": 10173.3115234375,
      "learning_rate": 4.6849315068493144e-05,
      "loss": 2.2554,
      "step": 1350
    },
    {
      "epoch": 3.481321494280458,
      "grad_norm": 14879.8046875,
      "learning_rate": 4.47945205479452e-05,
      "loss": 2.4265,
      "step": 1360
    },
    {
      "epoch": 3.5069194464442845,
      "grad_norm": 54159.07421875,
      "learning_rate": 4.273972602739726e-05,
      "loss": 2.5275,
      "step": 1370
    },
    {
      "epoch": 3.5325173986081113,
      "grad_norm": 4402.4931640625,
      "learning_rate": 4.068493150684931e-05,
      "loss": 2.5874,
      "step": 1380
    },
    {
      "epoch": 3.558115350771938,
      "grad_norm": 109784.9609375,
      "learning_rate": 3.8630136986301366e-05,
      "loss": 2.4885,
      "step": 1390
    },
    {
      "epoch": 3.5837133029357653,
      "grad_norm": 22076.89453125,
      "learning_rate": 3.657534246575342e-05,
      "loss": 2.4921,
      "step": 1400
    },
    {
      "epoch": 3.5837133029357653,
      "eval_loss": 2.3906149864196777,
      "eval_runtime": 71.6895,
      "eval_samples_per_second": 27.898,
      "eval_steps_per_second": 3.487,
      "step": 1400
    }
  ],
  "logging_steps": 10,
  "max_steps": 1560,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.328310437292933e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
